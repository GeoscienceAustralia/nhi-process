{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHIRS usage statistics\n",
    "\n",
    "This notebook provides a first-pass analysis of NHIRS geoserver request logs, to enable NHI to analyse who is using the service, how they are connecting, and trends in use. \n",
    "\n",
    "The log file is comprised of a timestamp and a JSON message that contains information on the request, which needs to be parsed to separate out the useful information.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* Presently there's only one dump of the log files from early September 2024. There's a [JIRA ticket with the Flying Hellfish](https://gajira.atlassian.net/browse/NHIRS-218) to create a process to save the logs on a regular basis.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* pandas\n",
    "* geopandas\n",
    "* cartopy\n",
    "* seaborn\n",
    "\n",
    "\n",
    "### Last updated:\n",
    "\n",
    "Craig Arthur\n",
    "September 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "PROJECTION = ccrs.PlateCarree(central_longitude=180)\n",
    "TRANSFORM = ccrs.PlateCarree()\n",
    "\n",
    "states = cfeature.NaturalEarthFeature(\n",
    "    category='cultural',\n",
    "    name='admin_1_states_provinces_lines',\n",
    "    scale='10m',\n",
    "    facecolor='none')\n",
    "LAND = cfeature.LAND()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r\"C:\\Users\\u12161\\Downloads\\logfiles.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We restrict to a subset of the URIs, so we focus on those that are requesting the geoserver endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uris = ['/geoserver/nhirs/ows', '/geoserver/nhirs/wfs', '/geoserver/nhirs/wms',\n",
    "        '/geoserver/access/ows', '/geoserver/access/wfs', '/geoserver/access/wms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a collection of functions to manipulate the messages. Some of these extract information from the JSON message, others clean strings to remove HTML ascii codes that appear in some parts of the messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string):\n",
    "    cleaned = string.replace('%27', '')\n",
    "    cleaned = re.sub(r\"^'|'$\", '', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def extractIp(jsondata):\n",
    "    try:\n",
    "        #logdata = json.loads(jsonstr)\n",
    "        return jsondata.get('httpRequest', {}).get('clientIp')\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractURI(jsondata):\n",
    "    try:\n",
    "        #logdata = json.loads(jsonstr)\n",
    "        return jsondata.get(\"httpRequest\", {}).get('uri')\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractAgent(jsondata):\n",
    "    try:\n",
    "        for d in jsondata.get('httpRequest', {}).get('headers'):\n",
    "            if d['name'] == 'User-Agent':\n",
    "                return ' '.join(d['value'].split(' ')[0:3])\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractLocation(ip):\n",
    "    try:\n",
    "        res = urlopen('https://ipinfo.io/' + ip + '/json')\n",
    "        data = json.load(res)\n",
    "        city = data['city']\n",
    "        lon = float(data['loc'].split(',')[0])\n",
    "        lat = float(data['loc'].split(',')[1])\n",
    "        return (city, lon, lat)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractService(jsondata):\n",
    "    try:\n",
    "        msg = jsondata.get('httpRequest', {}).get(\"args\").split(\"&\")\n",
    "\n",
    "        output = {item.split('=')[0]: item.split('=')[1:] for item in msg}\n",
    "        if \"SERVICE\" in output.keys():\n",
    "            return output['SERVICE'][0]\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractLayer(jsondata):\n",
    "    try:\n",
    "        msg = jsondata.get('httpRequest', {}).get(\"args\").split(\"&\")\n",
    "\n",
    "        output = {item.split('=')[0].upper(): item.split('=')[1:] for item in msg}\n",
    "        if \"LAYERS\" in output.keys():\n",
    "            return output['LAYERS'][0]\n",
    "        elif \"TYPENAME\" in output.keys():\n",
    "            return output['TYPENAME'][0]\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def extractEvent(jsondata):\n",
    "    \"\"\"\n",
    "    Extract the event id code (could be earthquake or TC)\n",
    "\n",
    "    :param jsondata: JSON data package\n",
    "    :return: event id code based on CQL filter string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        msg = jsondata.get('httpRequest', {}).get(\"args\").split(\"&\")\n",
    "\n",
    "        output = {item.split('=')[0].upper(): item.split('=')[1:] for item in msg}\n",
    "        if \"CQL_FILTER\" in output.keys():\n",
    "            if \"event_id\" in output['CQL_FILTER']:\n",
    "                return clean_string(output['CQL_FILTER'][1])\n",
    "            else:\n",
    "                return None\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file and iterate over all lines, splitting the line into a timestamp and the JSON message. Store these in a `pd.DataFrame`. We don't use `pd.read_csv` because there are a real mix of responses for each JSON package that includes variious combinations of commas, apostrophes, escape characters, etc. that make it impossible to read using `pd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdata = []\n",
    "timestamps = []\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        timestamps.append(pd.to_datetime(line.split(' ')[0]))\n",
    "        data = ' '.join(line.split(' ')[1:])\n",
    "        logdata.append(json.loads(data))\n",
    "\n",
    "df = pd.DataFrame(dict(\n",
    "    timestamp=timestamps,\n",
    "    message=logdata\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the functions to extract various attributes from the JSON message packets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['clientIp'] = df['message'].apply(extractIp)\n",
    "df['uri'] = df['message'].apply(extractURI)\n",
    "df['agent'] = df['message'].apply(extractAgent)\n",
    "df['service'] = df['message'].apply(extractService)\n",
    "df['layer'] = df['message'].apply(extractLayer)\n",
    "df['event_id'] = df['message'].apply(extractEvent)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour'] = df['timestamp'].dt.floor(\"h\")\n",
    "df['day'] = df['timestamp'].dt.floor('d')\n",
    "\n",
    "# Select the web service URIs:\n",
    "df = df.loc[df['uri'].isin(uris)]\n",
    "\n",
    "# Filter out my own IP (in fact this should be all GA addresses):\n",
    "df = df.loc[~df['clientIp'].str.startswith('124.47')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of hits from each unique IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clientIp'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of requests from different applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a chart of the hourly rate of requests across all endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.lineplot(df.set_index('timestamp').resample('h').size(), ax=ax, ) #marker='o', markersize=5)\n",
    "ax.set_ylabel(\"# requests per hour\")\n",
    "ax.set_xlabel(\"Time (UTC)\")\n",
    "dateloc = mdates.DayLocator(interval=5)\n",
    "datefmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax.xaxis.set_major_locator(dateloc)\n",
    "ax.grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a stacked bar chart of the daily requests, with separate categories for each URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgroup = df.groupby(['day', 'uri']).size().reset_index(name='request_count')\n",
    "df_pivot = dfgroup.pivot(index='day', columns='uri', values='request_count').fillna(0)\n",
    "df_pivot.index = df_pivot.index.date\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize=(12, 6))\n",
    "ax = df_pivot.plot(kind='bar', stacked=True, ax=ax, rot=90)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Requests')\n",
    "ax.set_title('Number of Requests per day by URI')\n",
    "dateloc = mdates.DayLocator(interval=2)\n",
    "datefmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax.xaxis.set_major_locator(dateloc)\n",
    "plt.legend(title='URI', ncols=2)\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['agent', 'clientIp']).size().reset_index(name='application count').to_csv(r\"X:\\georisk\\HaRIA_B_Wind\\projects\\NHIRS\\2. DATA\\1. Work Unit Assessment\\Usage\\20240911_Agent_IP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a chart of the daily number of unique IP adresses requesting services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of unique IP addresses by day:\n",
    "dfgroup = df.groupby(['day'])['clientIp'].nunique().reset_index(name='IP count').set_index('day')\n",
    "dfgroup.index = dfgroup.index.date\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize=(12, 6))\n",
    "ax = dfgroup.plot(kind='bar', stacked=False, ax=ax, rot=0)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('IP count')\n",
    "ax.set_title('Number of unique IPs')\n",
    "dateloc = mdates.DayLocator(interval=5)\n",
    "datefmt = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax.xaxis.set_major_locator(dateloc)\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLocation(ip):\n",
    "    try:\n",
    "        res = urlopen('https://ipinfo.io/' + ip + '/json')\n",
    "        data = json.load(res)\n",
    "        city = data['city']\n",
    "        lon = float(data['loc'].split(',')[0])\n",
    "        lat = float(data['loc'].split(',')[1])\n",
    "        return (city, lon, lat)\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we discover the location of the list of IP addresses so we can plot a map of where the services are being used. \n",
    "\n",
    "_Take care with this, as it may raise an access denied error if you provide too long a list_. This makes a request to an IP information page, and if we poll too many times in too short a period, the requests will be blocked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdf = df['clientIp'].value_counts().reset_index()['clientIp'].apply(extractLocation, )\n",
    "locdf = pd.DataFrame(tmpdf.tolist(), index=tmpdf.index, columns=['city', 'latitude', 'longitude'])\n",
    "locdf['clientIp'] = df['clientIp'].value_counts().index\n",
    "locgdf = gpd.GeoDataFrame(data=locdf, geometry=gpd.points_from_xy(locdf['longitude'], locdf['latitude']), crs=TRANSFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a map of the IP locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8),\n",
    "                       subplot_kw={'projection': PROJECTION})\n",
    "\n",
    "locgdf.plot(marker=\"*\", color='red', markersize=40, ax=ax, transform=TRANSFORM)\n",
    "ax.coastlines()\n",
    "ax.add_feature(states, edgecolor='0.15', linestyle='--')\n",
    "ax.add_feature(cfeature.LAND, edgecolor='k')\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "ax.set_extent((110, 160, -45, -10), crs=TRANSFORM) # Change this if you want to plot global map\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot table of the applications ('agent') that are used by each unique IP. Typically, there's only one application, but some IP addresses report more than one application, suggesting multiple users are making requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='clientIp', columns='agent', values='day', aggfunc='count').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
